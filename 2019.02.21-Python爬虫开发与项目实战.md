# Python爬虫开发与项目实战(学习笔记)

- 前面几章很多都是学习过的内容，快速浏览
- 参考书本和TLXY_study_note中的内容


# 1 第一章 回顾Python编程
- 案例放置在ch01之中

## 1.4 进程与线程

- 进程与线程的区别

- 进程：程序运行的一个状态
    - 包含地址空间，内存，数据栈
    - 每个进程有自己独立的运行环境
    - 多进程共享数据是一个问题

- 线程
    - 一个线程的独立运行片段，一个进程可以有多个线程
    - 轻量化的进程
    - 一个进程的多个线程共享数据和上下文运行环境
    - 共享互斥问题

- 多进程使用multiprocessing模块
- 多线程使用threading模块
    
 
### 1.4.1 多进程
- 使用multiprocessing模块创建多进程
    - 使用multiprocessing模块中的Process创建少量的多进程
    - 创建子进程，函数只需要执行函数的名称和执行函数的参数即可
    - 参考实例1.4.1 

- 大量进程，上千上万个进程使用multiprocessing模块中的Pool类
    - 创建进程池对象
    - Pool默认的进程数量是CPU的核数或者规定的进程数量
    - 如果池中的进程没有满，就会自动创建进程
    - 如果池中已满或者达到规定数量，进程就会等待，池中进程结束一个，才会创建新的进程 
    - 先创建Pool()实例，然后用apply_async()创建子进程
    - 进程池使用join()函数之前，需要先调用close()函数，然后所有任务就自动开始了，关闭某个进程
    - 相对于Process,不需要执行start()函数
    - 盗用close()之后，就不能再添加Process子进程了
    - 然后下一个进程在开始进程，具体参考实例
    - 参考实例1.4.2

- 进程之间的通信
    - 两个进程使用Pipe
    - 多个进程使用Queue
    - Queue实际就是一个安全队列
    - Queue提供了一个基本的FIFO容器,
    - Queue有两个方法Put和Get
    - put()方法向容器中存入数据，按顺序存入
    - get()方法从容器中取出数据，按循序取出
    - 参考实例1.4.3/1.4.4

- 队列补充知识：
    - queue.Queue()
    - 基本队列，先进先出
    - 参考1.4.3_1
    - LIFO队列，后进先出
    - 参考1.4.3_2

- multiprocessing.Queue()和queue.Queue()的区别
    - 参考：https://blog.csdn.net/u011318077/article/details/88089843
    - 参考：https://www.cnblogs.com/itogo/p/5635629.html
     
    
### 1.4.2 多线程
- threading模块
- 方法1：创建Thread实例
- 方法2：直接继承threading.Thread类
- 参考书中案例
- 参考TLXY_study_note中的高级语法，多线程

### 1.4.3 协程
- 可以使用yield生成器实现
- 推荐使用gevent包
- 参考实例1.4.5

- 处理大量的网络请求和并发出处理
- gevent提供的池，对并发数进行管理限制
- 使用pool.map执行任务
- 参考实例1.4.6

### 1.4.4 分布式进程
- multiprocessing模块
- managers子模块支持把多个进程分布到多台机器上
- 可以写一个服务进程作为调度者，将任务分布到其它多个进程中，然后通过网络通信进行管理
- 比如爬取图片：一般一个进程负责抓取图片的地址，将地址放在Queue（容器）队列中
- 另外一个进程负责从Queue队列中取出链接地址进行图片下载和存储到到本地
- 上述爬取图片的过程就可以做成分布式，一台机器负责获取链接，另外一台机器负责下载存储
- 上述问题核心：将Queue队列暴露到网络中，让其他机器可以访问

- 分布式进程的步骤
    - 建立Queue队列,负责进程之间的通信，任务队列task_queue,结果队列result_queue
    - 把第一步中的两个队列在网络中注册,注册时候将队列重新命名
    - 创建一个Queuemanager(BaseManager)的实例manager，相当于一个服务器，给定IP地址、端口和验证码
    - 启动实例manager
    - 访问Queue对象，即创建网络中暴露重命名后的Queue实例
    - 创建任务到本地队列中，自动上传任务到网络队列中，分配给任务进程进行处理
    - 任务进程先从网络中任务队列中取出任务，然后执行，将执行结果放入到网络中的结果队列中
    - 服务进程从结果队列中取出结果，直到执行完所有任务和取出所有的结果，任务进程关闭，然后服务进行关闭

- 先创建服务进程，再创建任务进程
- 参考实例1.4.7 1.4.8 运行正常
-
- 案例补充知识
- 知识补充1
    当我们在一台机器上写多进程程序时，创建的Queue可以直接拿来用，但是，在分布式多进程环境下，
    添加任务到Queue不可以直接对原始的task_queue进行操作，那样就绕过了QueueManager的封装，
    必须通过manager.get_task_queue()获得的Queue接口添加。
    然后，在另一台机器上启动任务进程（本机上启动也可以）

- 知识补充2
    其中task_queue和result_queue是两个队列，分别存放任务和结果。它们用来进行进程间通信，交换对象。
    因为是分布式的环境，放入queue中的数据需要等待Workers机器运算处理后再进行读取，
    QueueManager.register(‘get_task_queue’, callable=return_task_queue)
    QueueManager.register(‘get_result_queue’, callable=return_result_queue)
    这样就需要对queue用QueueManager进行封装放到网络中，这是通过上面的2行代码来实现的。
    我们给return_task_queue的网络调用接口取了一个名get_task_queue,
    而return_result_queue的名字是get_result_queue，
    方便区分对哪个queue进行操作。task.put(n)即是对task_queue进行写入数据，
    相当于分配任务。而result.get()即是等待workers机器处理后返回的结果。

- 知识补充3
    这个简单的Master/Worker模型有什么用？其实这就是一个简单但真正的分布式计算，把代码稍加改造，
    启动多个worker，就可以把任务分布到几台甚至几十台机器上，
    比如把计算n*n的代码换成发送邮件，就实现了邮件队列的异步发送。
    Queue对象存储在哪？注意到task_worker.py中根本没有创建Queue的代码，
    所以，Queue对象存储在taskManager.py进程中：
    参考图片分布式进程
    
    而Queue之所以能通过网络访问，就是通过QueueManager实现的。
    由于QueueManager管理的不止一个Queue，所以，要给每个Queue的网络调用接口起个名字，
    比如get_task_queue。task_worker这里的QueueManager注册的名字必须和task_manager中的一样。
    对比上面的例子，可以看出Queue对象从另一个进程通过网络传递了过来。
    只不过这里的传递和网络通信由QueueManager完成。

    authkey有什么用？这是为了保证两台机器正常通信，不被其他机器恶意干扰。
    如果task_worker.py的authkey和taskManager.py的authkey不一致，肯定连接不上。
    
    ---------------------
    参考以下博文： 
    原文：https://blog.csdn.net/u011318077/article/details/88094583 
    

    
## 1.5 网络编程
- TCP协议：服务器端与客户端要建立可靠的连接
- UDP协议：服务器端与客户端不需要建立连接
- 参考书本内容和TLXY_study_note中的高级语法，网络编程


# 2 第2章 Web前端基础
- 省略
- 参考书本内容和TLXY_study_note中的高级语法和Spider中的内容

# 3 第3章 初始网络爬虫
- 省略
- 参考书本内容和TLXY_study_note中的高级语法和Spider中的内容

# 4 第4章 HTML解析大法
- 省略
- TLXY_study_noteSpider中有详细的BS4使用教程和案例
- 参考书本内容和TLXY_study_note中的高级语法和Spider中的内容

# 5 第5章 数据存储
## 5.1 HTML正文抽取
### 5.1.1 HTML内容存储为JSON格式
- 参考实例5.1.1 
## 5.3 Email提醒
- Python发送邮件提醒
- 用途：爬虫自动爬取出现问题后可以自动发送提醒邮件
- 参考实例5.3

# 6 第6章 实战项目：基础爬虫
## 6.1 基础爬虫结构及运行流程
- 参考ch06中的实例
- HtmlParser解析器中，百度百科连接发生了变化进行了相应修改

# 7 第7章 实战项目：简单分布式爬虫
- 参考ch07中的实例
- NodeManager控制调度器，导入DataOutput和URLManager时候
    - 由于在同一个文件夹,并且添加了init文件，采用相对路径应该可以导入，但是导入出错
    - 将ch07文件加入系统路径，然后用绝对导入，可以解决问题
    - 参考博文：https://blog.csdn.net/u011318077/article/details/88061972
    
# 8 第8章 数据存储（数据库版）
- 参考书中内容
- 同时参考菜鸟教程：
- http://www.runoob.com/sqlite/sqlite-tutorial.html

# 9 第9章 动态网站抓取
- 9.2 动态网站采集需要分析网页请求中网络中的JS数据，比较复杂不推荐
- 9.3 PhantomJS已经不更新了，直接看9.4，推荐使用Selenium+Firefox/Chrome
    - 参考以前TLXY的动态HTML的笔记和实例
    - 参考我博文：https://blog.csdn.net/u011318077/article/details/86644430
    - 参考9.4中的实例
- 9.4 Selenium+Firefox操作浏览器，一些常用操作和查找方法
    - 参考实例
- 9.5 动态爬虫：爬去哪儿网

# 10 第10章 Web终端协议分析
- 网页登录POST分析
- 验证码问题
- 参考书本

# 11 第11章 终端协议分析
- PC客户端分析
- APP端抓包分析
- 两个工具：HTTP analyzer 和 Wireshark
- 关键通过抓包分析找到电脑客户端或者手机端的API接口，用于开始爬虫
- 实例：爬取酷我听书APP上的资源信息
- 看书和实例

# 12 第12-16章 看书和章节实例
- scrapy-redis分布式爬虫
    - 参考书中类容和以下网址
    - https://www.cnblogs.com/pythoner6833/p/9148937.html
    
    - 分布式爬虫，参考sht爬虫
        - 修改settings文件中的相关配制，主要是修改ITEM_PIPELINES和末尾中添加的内容
        - 修改sht_spider主程序，主要是类继承改变，起始url设置变化，爬虫程序中设置键的名称
        - 然后进入到服务器中设置键和值作为起始URL
        - 启动主程序，如果有新的URL，直接在服务器中设置，参考出ch16中的图片和pdf文件
        
        # 注意，改造成分布式爬虫后，传人URL，已经爬取过的ITEM会存储在本地服务器中
        # 可以在redis中输入keys * 查看所有的键
        # 使用flushall可以删除所有本地所有的键值数据
        # 删除数据后再次传入起始URL，然后启动爬虫，就可以开始爬取
        # 注意：分布式爬虫，redis服务器一直处于运行状态，爬虫不会自己结束，
        # 可以向服务器一直传入新的URL，然后爬虫会自动继续爬取新的URL，重复的会自动跳过
        
        - 爬取完成后，手动结束即可

    



    







